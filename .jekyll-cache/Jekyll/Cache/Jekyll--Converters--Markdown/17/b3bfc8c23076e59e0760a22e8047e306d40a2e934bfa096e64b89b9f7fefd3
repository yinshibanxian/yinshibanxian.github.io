I"™$<h2 id="game-tree-search">Game Tree Search</h2>

<h3 id="generalizing-search-problems">Generalizing search problems</h3>

<ul>
  <li>So far: our search problems have assumed agent has complete control of environment
    <ul>
      <li>state does not change unless the agent (robot) changes it</li>
    </ul>
  </li>
  <li>Assumption not always reasonable
    <ul>
      <li>other agents whose interests conï¬‚ict with yours</li>
    </ul>
  </li>
  <li>In these cases, we need to generalize our view of search to handle state changes that are not in the control of the agent</li>
</ul>

<h3 id="what-are-key-features-of-a-game">What are key features of a game</h3>

<ul>
  <li>Players have their own interests</li>
  <li>Each player tries to alter the world so as to best beneï¬t itself</li>
  <li>They are hard because: How you should play depends on how you think the other person will play; but how they play depends on how they think you will play</li>
</ul>

<h3 id="game-properties">Game properties</h3>

<ul>
  <li>Two-player</li>
  <li>Discrete: Game states or decisions can be mapped on discrete values.</li>
  <li>Finite: There are only a ï¬nite number of states and possible decisions that can be made</li>
  <li>Zero-sum (â€œ): Fully competitive
    <ul>
      <li>if one player wins, the other loses an equal amount</li>
      <li>note that some games donâ€™t have this property</li>
    </ul>
  </li>
  <li>Deterministic: no chance involved
    <ul>
      <li>no dice, or random deals of cards, or coin ï¬‚ips, etc.</li>
    </ul>
  </li>
  <li>Perfect information: all aspects of the state are fully observable
    <ul>
      <li>e.g., no hidden cards.</li>
    </ul>
  </li>
</ul>

<h3 id="extensive-form-two-player-zero-sum-games">Extensive Form Two-Player Zero-Sum Games</h3>

<ul>
  <li>But R,P,S is a simple /one shot0(ä¸€æ¬¡æ€§) game
    <ul>
      <li>single move each</li>
      <li>in game theory: a strategic or normal form game (ç­–ç•¥æˆ–èŒƒå¼åšå¼ˆ)</li>
    </ul>
  </li>
  <li>Many games extend over multiple moves
    <ul>
      <li>turn-taking: players act alternatively</li>
      <li>e.g., chess, checkers, etc.</li>
      <li>in game theory: extensive form games (æ‰©å±•å½¢å¼åšå¼ˆ)</li>
    </ul>
  </li>
  <li>Weâ€™ll focus on the extensive form
    <ul>
      <li>thatâ€™s where the computational questions emerges</li>
    </ul>
  </li>
</ul>

<h3 id="two-player-zero-sum-game--definition">Two-Player Zero-Sum Game â€“ Definition</h3>

<ul>
  <li>Two players A (Max) and B (Min)</li>
  <li>Set of states S (a finite set of states of the game)</li>
  <li>An initial stateIâˆˆS(where game begins)</li>
  <li>Terminal positionsTâŠ†S(Terminal states of the game:states where the game is over)</li>
  <li>Successors (or Succs - a function that takes a state as inputand returns a set of possible next states to whomever is dueto move)</li>
  <li>Utility (æ•ˆç›Š) or payoff (æ”¶ç›Š) functionV:Tâ†’R.  (amapping from terminal states to real numbers that show howgood is each terminal state for player A and bad for player B.)Y. LiuIntro to AI8 / 48</li>
</ul>

<h3 id="two-player-zero-sum-game--intuition">Two-Player Zero-Sum Game â€“ Intuition</h3>

<ul>
  <li>Players alternate moves (starting with A, or Max)
    <ul>
      <li>Game ends when some terminal t âˆŠT is reached</li>
    </ul>
  </li>
  <li>A game state: a state-player pair
    <ul>
      <li>Tells us what state weâ€™re in and whose move it is</li>
    </ul>
  </li>
  <li>Utility function and terminals replace goals
    <ul>
      <li>A, or Max, wants to maximize the terminal payoff</li>
      <li>B, or Min, wants to minimize the terminal payoff</li>
    </ul>
  </li>
  <li>Think of it as:
    <ul>
      <li>A, or Max, gets V(t)and B, or Min, gets â€“V(t) for terminal node t</li>
      <li>This is why itâ€™s called zero (or constant) sum</li>
    </ul>
  </li>
</ul>

<h2 id="the-minimax-strategy">The MiniMax Strategy</h2>

<ul>
  <li>Assume that the other player will always play their best move
    <ul>
      <li>you always play a move that will minimize the payoff thatcould be gained by the other player.</li>
      <li>By minimizing the other playerâ€™s payoff, you maximize yourown.</li>
    </ul>
  </li>
  <li>Note that if you know that Min will play poorly in somecircumstances, there might be a better strategy than MiniMax(i.e., a strategy that gives you a better payoff)</li>
  <li>Build full game tree (all leaves are terminals)
    <ul>
      <li>Root is start state, edges are possible moves, etc.</li>
      <li>Label terminal nodes with utilities</li>
    </ul>
  </li>
  <li>Back values upthe tree
    <ul>
      <li>U(t)is defined for all terminals (part of input)</li>
      <li>U(n)= min {U(c) : c isa child of n} if nis a Min node</li>
      <li>U(n)= max {U(c) : cis a child of n} if nis a Max node</li>
    </ul>
  </li>
</ul>

<h3 id="dfs-minmax">DFS Minmax</h3>

<ul>
  <li>Building the entire game tree and backing up values gives each player their strategy.</li>
  <li>However, the game tree is exponential in size.</li>
  <li>Furthermore, as we will see later it is not necessary to know all of the tree.</li>
  <li>To solve these problems we find a depth-firstimplementation of minimax.</li>
  <li>We run the depth-first search after each move to compute what is the next move for the MAXplayer. (We could do the same for the MINplayer).</li>
  <li>This avoids explicitly representing the exponentially sized game tree: we just compute each move as it is needed.</li>
</ul>

<h3 id="pruning">Pruning</h3>

<ul>
  <li>It is not necessary to examine entire tree to make correct MiniMax decision</li>
  <li>Assume depth-first generation of tree
    <ul>
      <li>After generating value for only someof nâ€™s children we can prove that weâ€™ll never reach n in a MiniMax strategy.</li>
      <li>So we neednâ€™t generate or evaluate any further children of n!</li>
    </ul>
  </li>
  <li>Two types of pruning (cuts):
    <ul>
      <li>pruning of max nodes (Î±-cuts)</li>
      <li>pruning of min nodes (Î²-cuts)</li>
    </ul>
  </li>
</ul>

<h3 id="cutting-max-nodes-alpha-cuts">Cutting Max Nodes ï¼ˆAlpha Cutsï¼‰</h3>

<ul>
  <li>At a Max node n:
    <ul>
      <li>Let Î²be the lowest value of nâ€™s siblings examined so far (siblings to the left of n that have already been searched)</li>
      <li>LetÎ±be the highest value of nâ€™s children examined so far (changes as children examined)</li>
    </ul>
  </li>
  <li>While at a Max node n, if Î±becomes â‰¥ Î²we can stop expanding the children of n
    <ul>
      <li>Min will never choose to move from nâ€™s parent to nsince it would choose one of nâ€™s lower valued siblings first</li>
    </ul>
  </li>
</ul>

<h2 id="practical-matters">Practical Matters</h2>

<p>â€œRealâ€ games are too large to enumerate tree</p>

<ul>
  <li>e.g., chess branching factor is roughly 35</li>
  <li>Depth 10 tree:  2,700,000,000,000,000 nodes</li>
  <li>Even alpha-beta pruning wonâ€™t help here!</li>
</ul>

<p>We must limit depth of search tree</p>

<ul>
  <li>Canâ€™t expand all the way to terminal nodes</li>
  <li>We must make heuristic estimates about the values of the(nonterminal) states at the leaves of the tree</li>
  <li>These heuristics are often called evaluation function</li>
</ul>

<h2 id="evaluation-functions--basic-requirements">Evaluation functions:  basic requirements</h2>

<ul>
  <li>Should order the terminal states in the same way as the trueutility function.</li>
  <li>The computation must not take too long!</li>
  <li>For nonterminal states, the evaluation function should bestrongly correlated with the actual chances of winning.</li>
</ul>

<h2 id="how-to-design-evaluation-functions">How to design evaluation functions</h2>

<ul>
  <li>Features of the states,e.g., in chess, the number of whitepawns(å’), black pawns, white queens, etc.</li>
  <li>The features, taken together, define various categories orequivalence classes of states:  the states in each category havethe same values for all the features.</li>
  <li>Any given category will contain some states that lead to wins,some that lead to draws, and some that lead to losses.</li>
  <li>e.g., suppose our experience suggests that 72% of the statesin a category lead to a win, 20% to a loss, and 8% to a draw.</li>
  <li>Then a reasonable evaluation for states in the category is theexpected utility value:0.72Â·1 + 0.20Â·(âˆ’1) + 0.08Â·0 = 0.52.</li>
  <li>However, there are too many categories</li>
  <li>Most evaluation functions compute separate numerical contributions from each feature and then combine them</li>
  <li>e.g., each pawn is worth 1, a knight(é©¬) or bishop(è±¡) isworth 3, a rook(è½¦)) 5, and the queen 9</li>
  <li>Mathematically, a weighted linear functionEval(s) =w1Â·f1(s) +â€¦+wnÂ·fn(s) =âˆ‘ni=1wiÂ·fi(s)</li>
  <li>Deep Blue used over 8000 features</li>
  <li>This involves a strong assumption:  the contribution of eachfeature is independent of the values of the other features.</li>
  <li>The assumption may not hold, hence nonlinear combinationsare also used</li>
  <li>The features and weights are not part of the rules of chess!</li>
  <li>They come from centuries of human chess-playing experience.</li>
  <li>In case this kind of experience is not available, the weights ofthe evaluation function can be estimated by machine learning techniques.</li>
</ul>
:ET