I"|<blockquote>
  <p>Consider a sparse matrix stored in the compressed row format (you may find a description of this format on the web or any suitable text on sparse linear algebra). Write an OpenMP program for computing the product of this matrix with a vector. Download sample matrics from the <a href="https://math.nist.gov/MatrixMarket/data/Harwell-Boeing/psadmit/1138_bus.html">Matrix Market</a> and test the performance of your implementation as a function of matrix size and number of threads.</p>
</blockquote>

<p>代码如下，需要开<code>c++11</code>。上面链接下载的矩阵是一个<em>1138 x 1138, 2596 entries</em>的稀疏矩阵，而向量是随机生成的一个<em>1138</em>维的稠密向量。一次乘法的时间可能不够明显，这里输出的是执行十万次矩阵乘法的时间。</p>
<pre><code class="language-cpp">#include &lt;bits/stdc++.h&gt;
#include &lt;omp.h&gt;
using namespace std;
typedef double lf;
typedef vector&lt;lf&gt; Vec;
typedef vector&lt;vector&lt;pair&lt;int, lf&gt;&gt;&gt; Mat;
int m, n, th;
Vec operator*(const Vec &amp;v, const Mat &amp;m)
{
	Vec r(m.size(), 0);
#pragma omp parallel for num_threads(th)
	for (int i = 0; i &lt; m.size(); ++i)
		for (const auto &amp;p : m[i])
			r[i] += v[p.first] * p.second;
	return r;
}
int main()
{
	ifstream fin("1138_bus.mtx");
	while (fin.peek() == '%')
		while (fin.get() != '\n')
			;
	fin &gt;&gt; m &gt;&gt; n &gt;&gt; th;
	Mat ma(m);
	for (int x, y, i = 0; i &lt; th; ++i)
	{
		lf t;
		fin &gt;&gt; x &gt;&gt; y &gt;&gt; t;
		ma[x - 1].emplace_back(y - 1, t);
	}
	Vec ve(n);
	for (int i = 0; i &lt; n; ++i)
		ve[i] = rand();
	cout &lt;&lt; "number of threads: ";
	cin &gt;&gt; th;
	auto begin = std::chrono::system_clock::now();
	for (int i = 1e5; i; --i)
		ve *ma;
	auto end = std::chrono::system_clock::now();
	std::chrono::duration&lt;double&gt; elapsed_seconds = end - begin;
	std::cout &lt;&lt; "elapsed time: " &lt;&lt; elapsed_seconds.count() &lt;&lt; "s\n";
}
</code></pre>

<table>
  <thead>
    <tr>
      <th>number of threads</th>
      <th>elapsed time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>12.1646s</td>
    </tr>
    <tr>
      <td>2</td>
      <td>8.52163s</td>
    </tr>
    <tr>
      <td>4</td>
      <td>9.68788s</td>
    </tr>
    <tr>
      <td>8</td>
      <td>9.74822s</td>
    </tr>
    <tr>
      <td>16</td>
      <td>13.2045s</td>
    </tr>
    <tr>
      <td>32</td>
      <td>19.4011s</td>
    </tr>
    <tr>
      <td>64</td>
      <td>31.9306s</td>
    </tr>
  </tbody>
</table>

<p>我所用的<em>VAIO Z Flip 2016</em>的CPU是<em>Intel(R) Core(TM) i7-6567U</em>，尽管挂着i7的名号但仍然是双核四线程的弱鸡。根据上面测试的结果可以发现，在2个线程时这个并行优化的向量矩阵乘法具有最高的加速比，多于16个线程时反而比1个线程的还要慢。</p>
<blockquote>
  <p>Implement a producer-consumer framework in OpenMP using sections to create a single producer task and a single consumer task. Ensure appropriate synchronization using locks. Test your program for a varying number of producers and consumers.</p>
</blockquote>

<p>使用上一个Project中实现的多线程访问的队列，已经加过锁了。</p>
<pre><code class="language-cpp">#include &lt;chrono&gt;
#include &lt;iostream&gt;
#include "wkMultiAccessQueue.hpp"
wk::MultiAccessQueue&lt;int&gt; q;
void producer(int cnt)
{
	for (int i = 0; i &lt; cnt; ++i)
		q.push(i);
}
void consumer(int cnt)
{
	for (int i = 0; i &lt; cnt; ++i)
		q.pop();
}
int main()
{
	int num;
	std::cout &lt;&lt; "number of producer-consumers: ";
	std::cin &gt;&gt; num;
	auto begin = std::chrono::system_clock::now();
#pragma omp parallel for
	for (int i = 0; i &lt; num; ++i)
		producer(1000);
#pragma omp parallel for
	for (int i = 0; i &lt; num; ++i)
		consumer(1000);
	auto end = std::chrono::system_clock::now();
	std::chrono::duration&lt;double&gt; elapsed_seconds = end - begin;
	std::cout &lt;&lt; "elapsed time: " &lt;&lt; elapsed_seconds.count() &lt;&lt; "s\n";
}
</code></pre>

<table>
  <thead>
    <tr>
      <th>number of producer-consumers</th>
      <th>elapsed time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>64</td>
      <td>0.0644229s</td>
    </tr>
    <tr>
      <td>512</td>
      <td>0.37481s</td>
    </tr>
    <tr>
      <td>4096</td>
      <td>2.96336s</td>
    </tr>
    <tr>
      <td>32768</td>
      <td>23.2893s</td>
    </tr>
  </tbody>
</table>
:ET