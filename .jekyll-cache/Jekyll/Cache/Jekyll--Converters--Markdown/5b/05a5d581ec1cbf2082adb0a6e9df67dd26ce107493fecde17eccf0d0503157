I"à4<h2 id="å®éªŒç›®çš„">å®éªŒç›®çš„</h2>

<p>åˆ©ç”¨pythonå®ç°kMeansç®—æ³•</p>

<h2 id="å®éªŒç®€ä»‹">å®éªŒç®€ä»‹</h2>

<p>åˆ©ç”¨ pythonçš„æ–‡æœ¬å¤„ç†èƒ½åŠ›å°†æ–‡æ¡£åˆ‡åˆ†æˆè¯ï¼Œé€šè¿‡é›†åˆå…ƒç´ çš„å”¯ä¸€æ€§ç”Ÿæˆè¯æ±‡åˆ— è¡¨ï¼ˆä¸åŒ…æ‹¬é‡å¤è¯æ±‡ï¼‰ï¼Œè¿›è€Œæ„å»ºè¯å‘é‡ï¼ˆè¯é›†å‘é‡æˆ–è¯è¢‹å‘é‡ï¼‰ï¼Œä»è¯å‘é‡è®¡ç®—æ¦‚ç‡ï¼Œç„¶å æ„å»ºåˆ†ç±»å™¨å¯¹é‚®ä»¶æ–‡æ¡£è¿›è¡Œåƒåœ¾é‚®ä»¶åˆ†ç±»ã€‚ä»£ç æ–‡ä»¶ï¼šbayes.py</p>

<h2 id="å®éªŒç¯å¢ƒ">å®éªŒç¯å¢ƒ</h2>

<h3 id="ç¡¬ä»¶">ç¡¬ä»¶</h3>

<p>æ‰€ç”¨æœºå™¨å‹å·ä¸ºVAIO Z Flip 2016</p>

<ul>
  <li>Intel(R) Core(TM) i7-6567U CPU @3.30GHZ 3.31GHz</li>
  <li>8.00GB RAM</li>
</ul>

<h3 id="è½¯ä»¶">è½¯ä»¶</h3>

<ul>
  <li>Windows 10, 64-bit (Build 17763) 10.0.17763</li>
  <li>Visual Studio Code 1.39.2
    <ul>
      <li>Python 2019.10.41019ï¼šä¹æœˆåº•å‘å¸ƒçš„VSCode Pythonæ’ä»¶æ”¯æŒåœ¨ç¼–è¾‘å™¨çª—å£å†…åŸç”Ÿè¿è¡Œjuyter nootbookäº†ï¼Œéå¸¸èµï¼</li>
      <li>Remote - WSL 0.39.9ï¼šé…åˆWSLï¼Œåœ¨Windowsä¸Šè·å¾—Linuxæ¥è¿‘åŸç”Ÿç¯å¢ƒçš„ä½“éªŒã€‚</li>
    </ul>
  </li>
  <li>Windows Subsystem for Linux [Ubuntu 18.04.2 LTS]ï¼šWSLæ˜¯ä»¥è½¯ä»¶çš„å½¢å¼è¿è¡Œåœ¨Windowsä¸‹çš„ Linuxå­ç³»ç»Ÿï¼Œæ˜¯è¿‘äº›å¹´å¾®è½¯æ¨å‡ºæ¥çš„æ–°å·¥å…·ï¼Œå¯ä»¥åœ¨Windowsç³»ç»Ÿä¸ŠåŸç”Ÿè¿è¡ŒLinuxã€‚
    <ul>
      <li>Python 3.7.4 64-bit (â€˜anaconda3â€™:virtualenv)ï¼šå®‰è£…åœ¨WSLä¸­ã€‚</li>
    </ul>
  </li>
</ul>

<h2 id="å®éªŒè¿‡ç¨‹">å®éªŒè¿‡ç¨‹</h2>

<ul>
  <li>åˆ©ç”¨ sklearnä¸­ BernoulliNBåˆ†ç±»è¯¥é‚®ä»¶æ•°æ®é›†</li>
  <li>bayes.pyä¸­çš„è¯­å¥â€œfrom numpy import * â€ç”¨è¯­å¥â€œimport numpy as npâ€ä»£æ›¿ï¼Œä¿®æ”¹å…¶ä¸­å¯¹åº”çš„ä»£ç ï¼Œä½¿å…¶èƒ½å¤Ÿæ­£å¸¸æ‰§è¡Œã€‚</li>
  <li>å°†è¯é›†å‘é‡ç”¨ TF-IDFè¯å‘é‡æ›¿ä»£ï¼Œæµ‹è¯•åˆ†æç»“æœ</li>
</ul>

<pre><code class="language-python"># åˆ©ç”¨ sklearnä¸­ BernoulliNBåˆ†ç±»è¯¥é‚®ä»¶æ•°æ®é›†
# bayes.pyä¸­çš„è¯­å¥â€œfrom numpy import * â€ç”¨è¯­å¥â€œimport numpy as npâ€ä»£æ›¿ï¼Œä¿®æ”¹å…¶ä¸­å¯¹åº”çš„ä»£ç ï¼Œä½¿å…¶èƒ½å¤Ÿæ­£å¸¸æ‰§è¡Œã€‚
# å°†è¯é›†å‘é‡ç”¨ TF-IDFè¯å‘é‡æ›¿ä»£ï¼Œæµ‹è¯•åˆ†æç»“æœ
# coding=utf-8
'''
é¡¹ç›®åç§°ï¼š
ä½œè€…
æ—¥æœŸ
'''

# å¯¼å…¥å¿…è¦åº“
from sklearn import feature_extraction  # å¯¼å…¥sklearnåº“, ä»¥è·å–æ–‡æœ¬çš„tf-idfå€¼
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
import numpy as np
from sklearn.naive_bayes import BernoulliNB

# åˆ›å»ºå®éªŒæ ·æœ¬


def loadDataSet():
    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                   ['mr', 'licks', 'ate', 'my', 'steak',
                       'how', 'to', 'stop', 'him'],
                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    # 1 ä»£è¡¨ä¾®è¾±æ€§æ–‡å­—, 0 ä»£è¡¨æ­£å¸¸è¨€è®º
    classVec = [0, 1, 0, 1, 0, 1]
    # postingListä¸ºè¯æ¡åˆ‡åˆ†åçš„æ–‡æ¡£é›†åˆï¼ŒclassVecä¸ºç±»åˆ«æ ‡ç­¾é›†åˆ
    return postingList, classVec


def createVocabList(dataSet):
    vocabSet = set([])
    for docment in dataSet:
        # ä¸¤ä¸ªé›†åˆçš„å¹¶é›†
        vocabSet = vocabSet | set(docment)
    # è½¬æ¢æˆåˆ—è¡¨
    return list(vocabSet)


def setOfWords2Vec(vocabList, inputSet):
    # åˆ›å»ºä¸€ä¸ªä¸è¯æ±‡è¡¨ç­‰é•¿çš„å‘é‡ï¼Œå¹¶å°†å…¶å…ƒç´ éƒ½è®¾ç½®ä¸º0
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            # æŸ¥æ‰¾å•è¯çš„ç´¢å¼•
            returnVec[vocabList.index(word)] = 1
        else:
            print("the word: %s is not in my vocabulary" % word)
    return returnVec


def train(trainMat, trainCategory):
    # trainMat:è®­ç»ƒæ ·æœ¬çš„è¯å‘é‡çŸ©é˜µï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªé‚®ä»¶çš„è¯å‘é‡
    # trainGategory:å¯¹åº”çš„ç±»åˆ«æ ‡ç­¾ï¼Œå€¼ä¸º0ï¼Œ1è¡¨ç¤ºæ­£å¸¸ï¼Œåƒåœ¾
    numTrain = len(trainMat)
    numWords = len(trainMat[0])
    pAbusive = sum(trainCategory)/float(numTrain)
    p0Num = np.ones(numWords)
    p1Num = np.ones(numWords)
    p0Denom = 2.0
    p1Denom = 2.0
    for i in range(numTrain):
        if trainCategory[i] == 1:

            p1Num += trainMat[i]

            p1Denom += sum(trainMat[i])
        else:

            p0Num += trainMat[i]

            p0Denom += sum(trainMat[i])
    # ç±»1ä¸­æ¯ä¸ªå•è¯çš„æ¦‚ç‡
    p1Vec = p1Num/p1Denom
    p0Vec = p0Num/p0Denom
    # ç±»0ä¸­æ¯ä¸ªå•è¯çš„æ¦‚ç‡
    return p0Vec, p1Vec, pAbusive


def classfy(vec2classfy, p0Vec, p1Vec, pClass1):
    p1 = sum(vec2classfy*p1Vec)+np.log(pClass1)
    p0 = sum(vec2classfy*p0Vec)+np.log(1-pClass1)
    if p1 &gt; p0:
        return 1
    else:
        return 0

# å¯¹é‚®ä»¶çš„æ–‡æœ¬åˆ’åˆ†æˆè¯æ±‡ï¼Œé•¿åº¦å°äº2çš„é»˜è®¤ä¸ºä¸æ˜¯è¯æ±‡ï¼Œè¿‡æ»¤æ‰å³å¯ã€‚è¿”å›ä¸€ä¸²å°å†™çš„æ‹†åˆ†åçš„é‚®ä»¶ä¿¡æ¯ã€‚


def textParse(bigString):
    import re
    listOfTokens = re.split(r'\W+', bigString)
    return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2]


def bagOfWords2Vec(vocabList, inputSet):
    # vocablistä¸ºè¯æ±‡è¡¨ï¼ŒinputSetä¸ºè¾“å…¥çš„é‚®ä»¶
    returnVec = [0]*len(vocabList)
    for word in inputSet:
        if word in vocabList:
            # æŸ¥æ‰¾å•è¯çš„ç´¢å¼•
            returnVec[vocabList.index(word)] = 1
        else:
            print("the word is not in my vocabulary")
    return returnVec

# å°†è¯é›†å‘é‡ç”¨ TF-IDFè¯å‘é‡æ›¿ä»£ï¼Œæµ‹è¯•åˆ†æç»“æœ

def TfidfVec(get_texts):
    mat = CountVectorizer()
    tf = TfidfTransformer()
    tfidf = tf.fit_transform(mat.fit_transform(get_texts))
    word = mat.get_feature_names()  # å•è¯çš„åç§°
    weight = tfidf.toarray()  # æƒé‡çŸ©é˜µ, åœ¨æ­¤ç¤ºèŒƒä¸­çŸ©é˜µä¸º(1, n)
    return weight


def spamTest():
    fullTest = []
    docList = []
    classList = []
    # it only 25 doc in every class
    for i in range(1, 26):
        wordList = textParse(open('email/spam/%d.txt' %
                                  i, encoding="ISO-8859-1").read())
        docList.append(wordList)
        fullTest.extend(wordList)
        classList.append(1)
        wordList = textParse(open('email/ham/%d.txt' %
                                  i, encoding="ISO-8859-1").read())
        docList.append(wordList)
        fullTest.extend(wordList)
        classList.append(0)
    # create vocabulary
    vocabList = createVocabList(docList)
    trainSet = list(range(50))
    testSet = []
    # choose 10 sample to test ,it index of trainMat
    for i in range(10):
        randIndex = int(np.random.uniform(0, len(trainSet)))  # num in 0-49
        testSet.append(trainSet[randIndex])
        del(trainSet[randIndex])
    trainMat = []
    trainClass = []
    for docIndex in trainSet:
        trainMat.append(bagOfWords2Vec(vocabList, docList[docIndex]))
        trainClass.append(classList[docIndex])
    # p0, p1, pSpam = train(np.array(trainMat), np.array(trainClass))
    # ä¿ç•™ä¸‹ä¸¤è¡Œè€Œå°†ä¸Šä¸€è¡Œæ³¨é‡Šæ‰ï¼Œå³å¯åˆ©ç”¨ sklearnä¸­ BernoulliNBåˆ†ç±»è¯¥é‚®ä»¶æ•°æ®é›†
    clf = BernoulliNB()
    clf.fit(np.array(trainMat), np.array(trainClass))
    errCount = 0
    for docIndex in testSet:
        wordVec = bagOfWords2Vec(vocabList, docList[docIndex])
        # if classfy(np.array(wordVec), p0, p1, pSpam) != classList[docIndex]:
        # ä¿ç•™ä¸‹ä¸¤è¡Œè€Œå°†ä¸Šä¸€è¡Œæ³¨é‡Šæ‰ï¼Œå³å¯åˆ©ç”¨ sklearnä¸­ BernoulliNBåˆ†ç±»è¯¥é‚®ä»¶æ•°æ®é›†
        if clf.predict(np.array([wordVec])) != classList[docIndex]:
            errCount += 1
            print(("classfication error"), docList[docIndex])

    print(("the error rate is "), float(errCount)/len(testSet))

if __name__ == '__main__':
    spamTest()
</code></pre>

<p>è¿è¡Œç»“æœå¦‚ä¸‹ï¼Œå¯ä»¥çœ‹åˆ°æ•ˆæœè¿˜æ˜¯å¾ˆä¸é”™çš„ã€‚</p>

<pre><code class="language-bash">the error rate is  0.0
</code></pre>

<ul>
  <li>ç¼–ç¨‹å®ç° PPTä¸­çš„ä¾‹ 1</li>
</ul>

<pre><code class="language-python"># ç¼–ç¨‹å®ç° PPTä¸­çš„ä¾‹ 1
import numpy as np
from sklearn.naive_bayes import GaussianNB

if __name__ == '__main__':
    X = np.array([[0,2,0,0],[0,2,0,1],[1,2,0,0],[2,1,0,0],[2,0,1,0],
                    [2,0,1,1],[1,0,1,1],[0,1,0,0],[0,0,1,0],[2,1,1,0],
                    [0,1,1,1],[1,1,0,1],[1,2,1,0],[2,1,0,1]])
    y = np.array([0,0,1,1,1,0,1,0,1,1,1,1,1,0])
    clf=GaussianNB()
    clf.fit(X, y)
    print(clf.predict([[0,1,1,0]]))
</code></pre>

<p>è¿è¡Œç»“æœå¦‚ä¸‹ã€‚</p>

<pre><code class="language-bash">[1]
</code></pre>

<ul>
  <li>åˆ©ç”¨æœ´ç´ è´å¶æ–¯ç®—æ³•å®ç°å¯¹ lab6çš„ä¸¤ä¸ªæ•°æ®é›†åˆ†ç±»ã€‚</li>
</ul>

<pre><code class="language-python"># åˆ©ç”¨æœ´ç´ è´å¶æ–¯ç®—æ³•å®ç°å¯¹ lab6çš„ä¸¤ä¸ªæ•°æ®é›†åˆ†ç±»ã€‚
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.naive_bayes import BernoulliNB

def file2matrix(filename):
    fr = open(filename)
    # å¾—åˆ°æ–‡ä»¶è¡Œæ•°
    arrayOfLines = fr.readlines()
    numberOfLines = len(arrayOfLines)
    # åˆ›å»ºè¿”å›çš„NumpyçŸ©é˜µ
    returnMat = np.zeros((numberOfLines, 3))
    classLabelVector = []
    # è§£ææ–‡ä»¶æ•°æ®åˆ°åˆ—è¡¨
    index = 0
    for line in arrayOfLines:
        line = line.strip()  # æ³¨é‡Š1
        listFromLine = line.split('\t')  # æ³¨é‡Š2
        returnMat[index, :] = listFromLine[0:3]
        classLabelVector.append(int(listFromLine[-1]))
        index += 1
    return returnMat, classLabelVector

if __name__ == '__main__':
    X, y = file2matrix('datingTestSet2.txt')
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    clf = BernoulliNB()
    clf.fit(X_train, y_train)
    print(clf.score(X_test, y_test, sample_weight=None))
</code></pre>

<p>è¿è¡Œç»“æœå¦‚ä¸‹ã€‚</p>

<pre><code class="language-bash">0.325
</code></pre>

<pre><code class="language-python"># åˆ©ç”¨æœ´ç´ è´å¶æ–¯ç®—æ³•å®ç°å¯¹ lab6çš„ä¸¤ä¸ªæ•°æ®é›†åˆ†ç±»ã€‚
# åˆ©ç”¨sklearnå®ç°ä½¿ç”¨æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨è¯†åˆ«æ‰‹å†™ä½“åº”ç”¨ã€‚
import numpy as np
from sklearn.naive_bayes import BernoulliNB
import time
from os import listdir


def img2vector(filename):
    '''
    filename:æ–‡ä»¶åå­—
    å°†è¿™ä¸ªæ–‡ä»¶çš„æ‰€æœ‰æ•°æ®æŒ‰ç…§é¡ºåºå†™æˆä¸€ä¸ªä¸€ç»´å‘é‡å¹¶è¿”å›
    '''
    returnVect = []
    fr = open(filename)
    for i in range(32):
        lineStr = fr.readline()
        for j in range(32):
            returnVect.append(int(lineStr[j]))
    return returnVect

# ä»æ–‡ä»¶åä¸­è§£æåˆ†ç±»æ•°å­—


def classnumCut(fileName):
    '''
    filename:æ–‡ä»¶å
    è¿”å›è¿™ä¸ªæ–‡ä»¶æ•°æ®ä»£è¡¨çš„å®é™…æ•°å­—
    '''
    fileStr = fileName.split('.')[0]
    classNumStr = int(fileStr.split('_')[0])
    return classNumStr

# æ„å»ºè®­ç»ƒé›†æ•°æ®å‘é‡åŠå¯¹åº”åˆ†ç±»æ ‡ç­¾å‘é‡


def trainingDataSet():
    '''
    ä»trainingDigitsæ–‡ä»¶å¤¹ä¸‹é¢è¯»å–æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼Œè¿”å›ï¼š
    trainingMatï¼šæ‰€æœ‰è®­ç»ƒæ•°æ®ï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ•°æ®æ–‡ä»¶ä¸­çš„å†…å®¹
    hwLabelsï¼šæ¯ä¸€é¡¹è¡¨ç¤ºtraningMatä¸­å¯¹åº”é¡¹çš„æ•°æ®åˆ°åº•ä»£è¡¨æ•°å­—å‡ 
    '''
    hwLabels = []
    # è·å–ç›®å½•traningDigitså†…å®¹(å³æ•°æ®é›†æ–‡ä»¶å)ï¼Œå¹¶å‚¨å­˜åœ¨ä¸€ä¸ªlistä¸­
    trainingFileList = listdir('trainingDigits')
    m = len(trainingFileList)  # å½“å‰ç›®å½•æ–‡ä»¶æ•°
    # åˆå§‹åŒ–mç»´å‘é‡çš„è®­ç»ƒé›†ï¼Œæ¯ä¸ªå‘é‡1024ç»´
    trainingMat = np.zeros((m, 1024))
    for i in range(m):
        fileNameStr = trainingFileList[i]
        # ä»æ–‡ä»¶åä¸­è§£æåˆ†ç±»æ•°å­—ï¼Œä½œä¸ºåˆ†ç±»æ ‡ç­¾
        hwLabels.append(classnumCut(fileNameStr))
        # å°†å›¾ç‰‡çŸ©é˜µè½¬æ¢ä¸ºå‘é‡å¹¶å‚¨å­˜åœ¨æ–°çš„çŸ©é˜µä¸­
        trainingMat[i, :] = img2vector('trainingDigits/%s' % fileNameStr)
    return hwLabels, trainingMat


def handwritingTest():
    # æ„å»ºè®­ç»ƒé›†
    hwLabels, trainingMat = trainingDataSet()

    # ä»testDigitsé‡Œé¢æ‹¿åˆ°æµ‹è¯•é›†
    testFileList = listdir('testDigits')

    # é”™è¯¯æ•°
    errorCount = 0.0

    # æµ‹è¯•é›†æ€»æ ·æœ¬æ•°
    mTest = len(testFileList)

    # è·å–ç¨‹åºè¿è¡Œåˆ°æ­¤å¤„çš„æ—¶é—´ï¼ˆå¼€å§‹æµ‹è¯•ï¼‰
    t1 = time.time()

    clf = BernoulliNB()
    clf.fit(trainingMat, hwLabels)

    for i in range(mTest):

        # å¾—åˆ°å½“å‰æ–‡ä»¶å
        fileNameStr = testFileList[i]

        # ä»æ–‡ä»¶åä¸­è§£æåˆ†ç±»æ•°å­—
        classNumStr = classnumCut(fileNameStr)

        # å°†å›¾ç‰‡çŸ©é˜µè½¬æ¢ä¸ºå‘é‡
        vectorUnderTest = img2vector('testDigits/%s' % fileNameStr)

        # è°ƒç”¨knnç®—æ³•è¿›è¡Œæµ‹è¯•
        classifierResult = clf.predict([vectorUnderTest])
        print("the classifier came back with: %d, the real answer is: %d" %
              (classifierResult, classNumStr))

        # é¢„æµ‹ç»“æœä¸ä¸€è‡´ï¼Œåˆ™é”™è¯¯æ•°+1
        if (classifierResult != classNumStr):
            errorCount += 1.0

    print("\nthe total number of tests is: %d" % mTest)
    print("the total number of errors is: %d" % errorCount)
    print("the total error rate is: %f" % (errorCount/float(mTest)))

    # è·å–ç¨‹åºè¿è¡Œåˆ°æ­¤å¤„çš„æ—¶é—´ï¼ˆç»“æŸæµ‹è¯•ï¼‰
    t2 = time.time()

    # æµ‹è¯•è€—æ—¶
    print("Cost time: %.2fmin, %.4fs." % ((t2-t1)//60, (t2-t1) % 60))


if __name__ == "__main__":
    handwritingTest()
</code></pre>

<p>è¿è¡Œæ—¶é—´å¦‚ä¸‹ã€‚</p>

<pre><code class="language-bash">the total number of tests is: 946
the total number of errors is: 65
the total error rate is: 0.068710
Cost time: 0.00min, 21.8384s.
</code></pre>

<h2 id="å®éªŒæ€»ç»“">å®éªŒæ€»ç»“</h2>

<p>é€šè¿‡æœ¬æ¬¡å®éªŒï¼Œæˆ‘å¤§è‡´ç†Ÿæ‚‰äº†sklearnä½¿ç”¨æœ´ç´ è´å¶æ–¯åšåˆ†ç±»çš„ç®—æ³•ï¼Œå¾—ç›Šäºä¹‹å‰æ¦‚ç‡è®ºçš„å­¦ä¹ ï¼Œè¿˜æ˜¯å¾ˆå®¹æ˜“ç†è§£å¹¶ä½¿ç”¨çš„ã€‚</p>
:ET