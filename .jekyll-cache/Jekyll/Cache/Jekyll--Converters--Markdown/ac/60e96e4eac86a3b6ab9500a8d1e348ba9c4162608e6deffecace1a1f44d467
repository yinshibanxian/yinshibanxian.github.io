I"ó4<h2 id="å®éªŒç®€ä»‹">å®éªŒç®€ä»‹</h2>

<p>ä½¿ç”¨ä¸‹é¢ä¸€ç§æˆ–å¤šç§ä¼˜åŒ–æ–¹æ³•å®ŒæˆCUDAçš„çŸ©é˜µå‘é‡ä¹˜æ³•$A\times b=C$,å…¶ä¸­Aæ˜¯$2^{14}\times 2^{14}$çš„æ–¹é˜µï¼Œ$b$ä¸º$2^{14}$ç»´å‘é‡ã€‚å‡è®¾çŸ©é˜µ$A$çš„å…ƒç´ ä¸º$a_{i,j}=i-0.1\times j+1$ï¼Œå‘é‡$b$çš„å…ƒç´ ä¸º$b_i=\log\sqrt{i\times i-i+2}$ã€‚</p>

<ul>
  <li>ä½¿ç”¨global memory</li>
  <li>ä½¿ç”¨åˆå¹¶è®¿å­˜</li>
  <li>ä½¿ç”¨constant memoryå­˜æ”¾å‘é‡</li>
  <li>ä½¿ç”¨shared memoryå­˜æ”¾å‘é‡å’ŒçŸ©é˜µ</li>
</ul>

<h2 id="å®éªŒç¯å¢ƒ">å®éªŒç¯å¢ƒ</h2>

<p>å®éªŒåœ¨è€å¸ˆæä¾›çš„è®¡ç®—é›†ç¾¤çš„ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè¿›è¡Œã€‚å•èŠ‚ç‚¹çš„æ˜¾å¡é…ç½®å¦‚ä¸‹ï¼š</p>

<pre><code class="language-bash">$ nvdia-smi
Mon Dec  2 08:38:49 2019
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0    24W / 250W |      0MiB / 16130MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>

<h2 id="å®éªŒåŸç†">å®éªŒåŸç†</h2>

<p>ä¼˜åŒ–CUDAæ¶æ„ä¸Šçš„ç¨‹åºï¼Œä¸€èˆ¬ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è€ƒè™‘ï¼š</p>

<ul>
  <li>é€‰æ‹©å¥½çš„å¹¶è¡Œç®—æ³•ï¼Œå‘æ˜æ›´å¤šçš„æ•°æ®å¹¶è¡Œæ€§</li>
  <li>ä¿æŒSMå°½å¯èƒ½å¿™ç¢Œï¼Œå°½é‡åˆ©ç”¨æ‰€æœ‰çš„SMå‚ä¸è®¡ç®—
    <ul>
      <li>åŠ å¤§æ•°æ®é‡</li>
      <li>å‡å°çº¿ç¨‹å—å¤§å°</li>
    </ul>
  </li>
  <li>ä¼˜åŒ–å­˜å‚¨å™¨çš„ä½¿ç”¨
    <ul>
      <li>å…¨å±€å­˜å‚¨å™¨åˆå¹¶è®¿é—®</li>
      <li>ä½¿ç”¨æ›´å¿«çš„constant memoryæˆ–shared memory</li>
    </ul>
  </li>
</ul>

<h2 id="å®éªŒè¿‡ç¨‹">å®éªŒè¿‡ç¨‹</h2>

<p>ç”±äºéƒ½æ˜¯CUDAæ¶æ„ä¸Šçš„æ ¸å‡½æ•°å¯¹æ¯”æ€§èƒ½ï¼Œä¸‹é¢çš„è®¡æ—¶éƒ½åªæµ‹äº†ç”¨äºæ ¸å‡½æ•°è®¡ç®—çš„æ—¶é—´ï¼Œè€Œä¸åŒ…å«æ•°æ®æ‹·è´çš„éƒ¨åˆ†ï¼ˆå¦åˆ™è¿è¡Œæ—¶é—´éƒ½åœ¨300mså·¦å³ï¼ŒåŸºæœ¬ä¸Šéƒ½æ˜¯æ‹·è´çš„æ—¶é—´è€Œæ²¡æœ‰å‚è€ƒä»·å€¼äº†ï¼‰ã€‚å½“ç„¶ï¼Œç”±äºæ²¡æœ‰è®¡å…¥æ‹·è´ç­‰é¢„å¤„ç†çš„æ—¶é—´ï¼Œé‚£äº›éœ€è¦è®¡ç®—è½¬ç½®æˆ–è€…é¢„è¯»å–çš„ç®—æ³•åœ¨è¿™é‡Œä¼šæœ‰ä¼˜åŠ¿ä¸€äº›ã€‚</p>

<h3 id="ä½¿ç”¨global-memory">ä½¿ç”¨global memory</h3>

<p>è¿™æ˜¯æœ€åŸºç¡€çš„çŸ©é˜µå‘é‡ä¹˜æ³•ã€‚è¿™é‡Œå‡è®¾çº¿ç¨‹å—éƒ½æ˜¯ä¸€ç»´ç»„ç»‡çš„ï¼Œæ¯ä¸ªCUDAçº¿ç¨‹è®¡ç®—çŸ©é˜µçš„ä¸€è¡Œä¸å‘é‡ä¹˜ç§¯ï¼Œè¿™æ ·å„çº¿ç¨‹ä¹‹é—´æ²¡æœ‰è¯»å†™å†²çªï¼Œä¸éœ€è¦ä½¿ç”¨åŸå­æ“ä½œã€‚</p>

<pre><code class="language-c">void __global__ MatVecMulGlobalMemory(const lf *A, const lf *B, lf *C, const size_t nRow, const size_t nCol)
{
	const size_t i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i &lt; nRow)
	{
		lf res = 0; //å°†ç»“æœå…ˆå­˜åœ¨å¯„å­˜å™¨é‡Œï¼Œå‡å°‘å¯¹å‘é‡Cçš„è®¿å­˜
		for (size_t j = 0; j &lt; nCol; ++j)
			res += A[i * nCol + j] * B[j];
		C[i] = res;
	}
}
</code></pre>

<p>è¿è¡Œæ—¶é—´ä¸º<code>3.861504ms</code>ã€‚</p>

<h3 id="ä½¿ç”¨åˆå¹¶è®¿å­˜">ä½¿ç”¨åˆå¹¶è®¿å­˜</h3>

<p>æ‰€è°“åˆå¹¶è®¿å­˜ï¼ŒæŒ‡çš„æ˜¯ç›¸é‚»çš„çº¿ç¨‹è®¿é—®æ®µå¯¹é½çš„åœ°å€ã€‚æ¯”å¦‚åœ¨ä¹‹å‰çš„ä»£ç ä¸­ï¼Œ<code>j == 0</code>æ—¶çº¿ç¨‹0è®¿é—®<code>A[0]</code>ï¼Œçº¿ç¨‹1è®¿é—®<code>A[nCol]</code>ï¼Œçº¿ç¨‹2è®¿é—®<code>A[2 * nCol]</code>â€¦å®ƒä»¬å¹¶ä¸ç›¸é‚»ï¼Œå› æ­¤ä¸æ»¡è¶³åˆå¹¶è®¿é—®çš„è¦æ±‚ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æŠŠåŸæ¥çš„çŸ©é˜µ$A$æ±‚å‡ºè½¬ç½®$A^T$ï¼Œæ­¤æ—¶<code>j == 0</code>æ—¶çº¿ç¨‹0è®¿é—®<code>At[0]</code>ï¼Œçº¿ç¨‹1è®¿é—®<code>At[1]</code>ï¼Œçº¿ç¨‹2è®¿é—®<code>At[2]</code>â€¦æ­¤æ—¶æ»¡è¶³äº†åˆå¹¶è®¿é—®çš„è¦æ±‚ã€‚</p>

<pre><code class="language-c">void __global__ MatVecMulGlobalMemoryAlign(const lf *At, const lf *B, lf *C, const size_t nRow, const size_t nCol)
{
	const size_t i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i &lt; nRow)
	{
		lf res = 0;
		for (size_t j = 0; j &lt; nCol; ++j)
			res += At[j * nRow + i] * B[j];
		C[i] = res;
	}
}
</code></pre>

<p>è¿è¡Œæ—¶é—´ä¸º<code>1.570816ms</code>ï¼Œæ€§èƒ½æé«˜äº†å°†è¿‘ä¸€å€ï¼Œå……åˆ†è¯´æ˜äº†åˆå¹¶è®¿å­˜çš„é‡è¦æ€§ã€‚</p>

<h3 id="ä½¿ç”¨constant-memoryå­˜æ”¾å‘é‡">ä½¿ç”¨constant memoryå­˜æ”¾å‘é‡</h3>

<p>æ³¨æ„åˆ°å‘é‡åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ä¸ä¼šæ”¹å˜ï¼Œä¸”æ¯ä¸ªçº¿ç¨‹è®¿é—®ç›¸åŒåœ°å€ï¼Œå› æ­¤è€ƒè™‘æŠŠå®ƒæ”¾åœ¨constant memoryä¸­ã€‚</p>

<p>NVIDIAç¡¬ä»¶æä¾›äº†64KBçš„å¸¸é‡å†…å­˜ï¼Œå¹¶ä¸”å¸¸é‡å†…å­˜é‡‡ç”¨äº†ä¸åŒäºæ ‡å‡†å…¨å±€å†…å­˜çš„å¤„ç†æ–¹å¼ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å¤§å°ä¸º$2^{14}$çš„å•ç²¾åº¦æµ®ç‚¹æ•°å‘é‡$b$å¤§å°æ°å¥½ä¸º64KBï¼Œæ­£å¥½å¯ä»¥å®Œæ•´ä¿å­˜ã€‚å¦‚æœå‘é‡è¶…è¿‡äº†constant memoryçš„64KBä¸Šé™ï¼Œé‚£å°±éœ€è¦åˆ†æ‰¹è¿›è¡Œï¼Œå¤šæ¬¡ä¼ è¾“å’Œå¯åŠ¨å†…æ ¸ã€‚</p>

<pre><code class="language-c">lf __constant__ d_Bc[(1 &lt;&lt; 16) / sizeof(lf)]; //64KB
void __global__ MatVecMulConstantMemory(const lf *At, const lf *B, lf *C, const size_t nRow, const size_t nCol)
{
	const size_t i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i &lt; nRow)
	{
		lf res = 0;
		for (size_t j = 0; j &lt; nCol; ++j)
			res += At[j * nRow + i] * d_Bc[j];
		C[i] = res;
	}
}
</code></pre>

<p>è¿è¡Œæ—¶é—´ä¸º<code>1.536000ms</code>ï¼Œåœ¨ä¸Šä¸€æ­¥çš„åŸºç¡€ä¸Šç•¥å¾®æé«˜ã€‚ä½¿ç”¨å¸¸é‡å†…å­˜å¯ä»¥æå‡è¿ç®—æ€§èƒ½çš„åŸå› ä¸»è¦æœ‰ä¸¤ä¸ªï¼š</p>

<ol>
  <li>å¯¹å¸¸é‡å†…å­˜çš„å•æ¬¡è¯»æ“ä½œå¯ä»¥å¹¿æ’­åˆ°åŒä¸ªåŠçº¿ç¨‹æŸçš„å…¶ä»–$15$ä¸ªçº¿ç¨‹ï¼Œè¿™ç§æ–¹å¼äº§ç”Ÿçš„å†…å­˜æµé‡åªæ˜¯ä½¿ç”¨å…¨å±€å†…å­˜æ—¶çš„$\frac{1}{16}$ã€‚</li>
  <li>ç¡¬ä»¶å°†ä¸»åŠ¨æŠŠå¸¸é‡æ•°æ®ç¼“å­˜åœ¨GPUä¸Šã€‚åœ¨ç¬¬ä¸€æ¬¡ä»å¸¸é‡å†…å­˜çš„æŸä¸ªåœ°å€ä¸Šè¯»å–åï¼Œå½“å…¶ä»–åŠçº¿ç¨‹æŸè¯·æ±‚åŒä¸€ä¸ªåœ°å€æ—¶ï¼Œé‚£ä¹ˆå°†å‘½ä¸­ç¼“å­˜ï¼Œè¿™åŒæ ·å‡å°‘äº†é¢å¤–çš„å†…å­˜æµé‡ã€‚</li>
</ol>

<h3 id="ä½¿ç”¨shared-memoryå­˜æ”¾å‘é‡å’ŒçŸ©é˜µ">ä½¿ç”¨shared memoryå­˜æ”¾å‘é‡å’ŒçŸ©é˜µ</h3>

<p>å¯¹äºblockå†…å†…å­˜æ¥è¯´ï¼Œå‘é‡éƒ½æ˜¯å…±äº«çš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¯”constant memoryæ›´å¿«çš„shared memoryæ¥å­˜å‚¨ï¼Œæ­¤æ—¶ç›¸æ¯”è¾ƒä½¿ç”¨å¸¸é‡å†…å­˜ï¼Œæˆ‘ä»¬å…æ‰äº†å‘é‡æ¯”è¾ƒå¤§çš„æ—¶å€™å¤šæ¬¡æ•°æ®æ‹·è´å’Œå¯åŠ¨æ ¸å‡½æ•°çš„å¼€é”€ï¼Œä¹Ÿæ²¡æœ‰ä½¿ç”¨å…¨å±€å˜é‡ï¼Œå¢åŠ äº†ä»£ç çš„å¯æ‰©å±•æ€§ã€‚å½“ç„¶ï¼Œshared memoryæ›´å°ï¼Œå› æ­¤éœ€è¦å¯¹å‘é‡è¿›è¡Œåˆ†å—å¤„ç†ã€‚</p>

<p>å¦å¤–éœ€è¦æ›´æ­£çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå¹¶ä¸éœ€è¦ä½¿ç”¨shared memoryå»å­˜çŸ©é˜µï¼Œå› ä¸ºåœ¨è¿™ä¸ªçŸ©é˜µå‘é‡ä¹˜çš„è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªçŸ©é˜µå…ƒç´ åªè¢«è®¿é—®äº†ä¸€æ¬¡ã€‚æ­¤å¤–ï¼Œshared memoryçš„å¤§å°ä¹Ÿå¹¶ä¸è¶³ä»¥å­˜ä¸‹å®Œæ•´çš„çŸ©é˜µï¼ˆç”šè‡³æ˜¯å‘é‡ï¼‰ã€‚</p>

<pre><code class="language-c">void __global__ MatVecMulSharedMemory(const lf *At, const lf *B, lf *C, const size_t nRow, const size_t nCol)
{
	extern lf __shared__ Bs[];
	const size_t i = blockDim.x * blockIdx.x + threadIdx.x;
	lf res = 0;
	for (size_t jBeg = 0, jEnd = blockDim.x &lt; nCol ? blockDim.x : nCol;
		 jBeg &lt; nCol;
		 jBeg += blockDim.x, jEnd += blockDim.x)
	{
		__syncthreads(); //é˜²æ­¢æœ‰çš„è¿›ç¨‹è¿˜åœ¨è¯»Bs
		if (jBeg + threadIdx.x &lt; nCol)
			Bs[threadIdx.x] = B[jBeg + threadIdx.x];
		__syncthreads();
		if (i &lt; nRow)
			for (size_t j = jBeg; j &lt; jEnd; ++j)
				res += At[j * nRow + i] * Bs[j - jBeg];
	}
	if (i &lt; nRow)
		C[i] = res;
}
</code></pre>

<p>è¿è¡Œæ—¶é—´ä¸º<code>1.821696ms</code>ï¼Œåè€Œæœ‰ä¸€å®šçš„ä¸‹é™ã€‚åˆ†æä¸€ä¸‹åŸå› ï¼š</p>

<ol>
  <li>ä½¿ç”¨å¸¸é‡å†…å­˜æ—¶ï¼Œå°†åŸæ¥çš„æ•°æ®æ‹·è´åˆ°å¸¸é‡å†…å­˜çš„æ—¶é—´å¹¶æ²¡æœ‰è¢«ç®—è¿›å»ï¼Œè€Œè¿™é‡Œè¯»å†…å­˜çš„è¿‡ç¨‹æ˜¯åœ¨æ ¸å‡½æ•°å†…éƒ¨çš„ã€‚æƒ³æ¥å¦‚æœåœ¨æ›´å¤§çš„çŸ©é˜µä¸Šè¿›è¡Œè®¡ç®—ï¼Œä½¿ç”¨shared memoryåº”è¯¥ä¼šæœ‰æ›´å¥½çš„è¡¨ç°ã€‚</li>
  <li>è€å¸ˆé›†ç¾¤ä¸Šçš„æ˜¾å¡æ€§èƒ½è¿‡äºå¼ºæ‚ï¼ˆåœ¨ä»Šå¹´åä¸€æœˆSCè¶…ç®—å¤§ä¼šåˆšå‘å¸ƒTesla V100Så‰ï¼ŒTesla V100ä¸€ç›´éƒ½æ˜¯å¸‚é¢èƒ½ä¹°åˆ°çš„æœ€å¼ºç®—åŠ›ï¼‰ï¼Œå†…å­˜è¯»å†™æ€§èƒ½æ¯”ä»¥å¾€çš„æ˜¾å¡éƒ½è¦å¼ºå¾ˆå¤šï¼Œå› æ­¤å¯¹æœ¬æ¥å·²ç»å¾ˆå¿«çš„global memoryçš„ä¼˜åŒ–æ•ˆæœæ²¡æœ‰é‚£ä¹ˆæ˜æ˜¾äº†ï¼Œè€Œå¯¹åº”çš„æ ¸å‡½æ•°å´æ¯”æœ´ç´ çš„ç®—æ³•æ›´å¤æ‚ï¼Œä¸€å®šç¨‹åº¦ä¸Šå¢åŠ äº†è¿è¡Œçš„æ—¶é—´å¸¸æ•°ã€‚</li>
</ol>

<h3 id="matvecmulo12727"><code>MatVecMul.o12727</code></h3>

<p>åˆ†åˆ«æ˜¯ä¸Šé¢å››ä¸ªæ ¸å‡½æ•°çš„è¿è¡Œæ—¶é—´ã€‚</p>

<pre><code class="language-bash">3.861504ms
1.570816ms
1.536000ms
1.821696ms
</code></pre>

<p>å¯ä»¥çœ‹åˆ°ï¼Œç”±äºç°åœ¨çš„ç¡¬ä»¶æ€§èƒ½å·²ç»å¤§å¤§å¼ºäºæ•°å¹´å‰ï¼Œåšå­˜å‚¨å™¨çš„ä¼˜åŒ–æ•ˆæœå·²ç»æ¯”è¾ƒå°ï¼ˆå¹¶ä¸æ˜¯è¯´æ²¡æœ‰ï¼‰ã€‚å› æ­¤ï¼Œå¯¹è¿™ä¸ªé—®é¢˜æ¥è¯´ï¼Œæœ€ä¸»è¦çš„æ˜¯è¦é€‰ä¸€ä¸ªä¼˜ç§€çš„å¹¶è¡Œç®—æ³•ï¼Œå¹¶å¯¹ç¨‹åºä»£ç åšå¥½è®¿å­˜åˆ†æå’Œä¼˜åŒ–ã€‚</p>

<p>å½“ç„¶ä¹Ÿä¸æ˜¯è¯´å­˜å‚¨å™¨ç»“æ„å°±ä¸å†é‡è¦ï¼Œè¿˜æ˜¯è¦å…·ä½“é—®é¢˜å…·ä½“åˆ†æã€‚ä¸Šé¢å¾ˆå¤šç®—æ³•éƒ½æ˜¯è¦å¯¹çŸ©é˜µæˆ–è€…å‘é‡è¿›è¡Œé¢„å¤„ç†çš„ï¼Œè€Œå¹¶æ²¡æœ‰æŠŠå¯¹åº”çš„ä»£ä»·ï¼ˆæ—¶é—´ã€å†…å­˜ç©ºé—´ã€å¯æ‰©å±•æ€§ç­‰ï¼‰è®¡å…¥åœ¨å†…ï¼Œå®é™…ä¸Šåœ¨è¿ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒçš„æ—¶å€™è¿™äº›ä»ç„¶æ˜¯å¿…é¡»è¦è€ƒè™‘çš„ã€‚</p>

<h3 id="matvecmulpbs"><code>MatVecMul.pbs</code></h3>

<p>è°ƒåº¦è„šæœ¬ã€‚</p>

<pre><code class="language-bash">#PBS -N MatVecMul
#PBS -l nodes=1:ppn=32:gpus=1
#PBS -j oe
#PBS -q gpu
source /public/software/profile.d/cuda10.0.sh
cd $PBS_O_WORKDIR
nvcc MatVecMul.cu -o MatVecMul
./MatVecMul
</code></pre>

<h3 id="matvecmulcu"><code>MatVecMul.cu</code></h3>

<p>å®Œæ•´ä»£ç ã€‚</p>

<pre><code class="language-c">#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;math.h&gt;
#include &lt;cuda_runtime.h&gt;
typedef float lf;
void __global__ MatVecMulGlobalMemory(const lf *A, const lf *B, lf *C, const size_t nRow, const size_t nCol)
{
	const size_t i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i &lt; nRow)
	{
		lf res = 0; //å°†ç»“æœå…ˆå­˜åœ¨å¯„å­˜å™¨é‡Œï¼Œå‡å°‘å¯¹å‘é‡Cçš„è®¿å­˜
		for (size_t j = 0; j &lt; nCol; ++j)
			res += A[i * nCol + j] * B[j];
		C[i] = res;
	}
}
void __global__ MatVecMulGlobalMemoryAlign(const lf *At, const lf *B, lf *C, const size_t nRow, const size_t nCol)
{
	const size_t i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i &lt; nRow)
	{
		lf res = 0;
		for (size_t j = 0; j &lt; nCol; ++j)
			res += At[j * nRow + i] * B[j];
		C[i] = res;
	}
}
lf __constant__ d_Bc[(1 &lt;&lt; 16) / sizeof(lf)]; //64KB
void __global__ MatVecMulConstantMemory(const lf *At, const lf *B, lf *C, const size_t nRow, const size_t nCol)
{
	const size_t i = blockDim.x * blockIdx.x + threadIdx.x;
	if (i &lt; nRow)
	{
		lf res = 0;
		for (size_t j = 0; j &lt; nCol; ++j)
			res += At[j * nRow + i] * d_Bc[j];
		C[i] = res;
	}
}
void __global__ MatVecMulSharedMemory(const lf *At, const lf *B, lf *C, const size_t nRow, const size_t nCol)
{
	extern lf __shared__ Bs[];
	const size_t i = blockDim.x * blockIdx.x + threadIdx.x;
	lf res = 0;
	for (size_t jBeg = 0, jEnd = blockDim.x &lt; nCol ? blockDim.x : nCol;
		 jBeg &lt; nCol;
		 jBeg += blockDim.x, jEnd += blockDim.x)
	{
		__syncthreads(); //é˜²æ­¢æœ‰çš„è¿›ç¨‹è¿˜åœ¨è¯»Bs
		if (jBeg + threadIdx.x &lt; nCol)
			Bs[threadIdx.x] = B[jBeg + threadIdx.x];
		__syncthreads();
		if (i &lt; nRow)
			for (size_t j = jBeg; j &lt; jEnd; ++j)
				res += At[j * nRow + i] * Bs[j - jBeg];
	}
	if (i &lt; nRow)
		C[i] = res;
}
int main()
{
	const size_t
		nRow = 1 &lt;&lt; 14,
		nCol = 1 &lt;&lt; 14;
	lf
		*h_A = (lf *)malloc(sizeof(lf) * nRow * nCol),
		*h_At = (lf *)malloc(sizeof(lf) * nRow * nCol),
		*h_B = (lf *)malloc(sizeof(lf) * nCol),
		*h_C = (lf *)malloc(sizeof(lf) * nRow),
		*d_A,
		*d_At,
		*d_B,
		*d_C;
	for (size_t i = 0; i &lt; nRow; ++i)
		for (size_t j = 0; j &lt; nCol; ++j)
			h_A[i * nCol + j] = i - 0.1 * j + 1;
	for (size_t j = 0; j &lt; nCol; ++j)
	{
		h_B[j] = log(sqrt(j * j - j + 2));
		for (size_t i = 0; i &lt; nRow; ++i)
			h_At[j * nRow + i] = i - 0.1 * j + 1;
	}

	cudaMalloc((lf **)&amp;d_A, sizeof(lf) * nRow * nCol);
	cudaMalloc((lf **)&amp;d_At, sizeof(lf) * nRow * nCol);
	cudaMalloc((lf **)&amp;d_B, sizeof(lf) * nCol);
	cudaMalloc((lf **)&amp;d_C, sizeof(lf) * nRow);

	cudaMemcpy(d_A, h_A, sizeof(lf) * nRow * nCol, cudaMemcpyHostToDevice);
	cudaMemcpy(d_At, h_At, sizeof(lf) * nRow * nCol, cudaMemcpyHostToDevice);
	cudaMemcpy(d_B, h_B, sizeof(lf) * nCol, cudaMemcpyHostToDevice);
	cudaMemcpyToSymbol(d_Bc, h_B, sizeof(lf) * nCol, cudaMemcpyHostToDevice);

	for (int i = 0; i &lt; 4; ++i)
	{
		cudaEvent_t beg, end;
		cudaEventCreate(&amp;beg);
		cudaEventCreate(&amp;end);
		cudaEventRecord(beg, 0);
		size_t blocks = 1 &lt;&lt; 7, grids = (nRow + blocks - 1) / blocks;
		if (i == 0)
			MatVecMulGlobalMemory&lt;&lt;&lt;grids, blocks&gt;&gt;&gt;(d_A, d_B, d_C, nRow, nCol);
		else if (i == 1)
			MatVecMulGlobalMemoryAlign&lt;&lt;&lt;grids, blocks&gt;&gt;&gt;(d_At, d_B, d_C, nRow, nCol);
		else if (i == 2)
			MatVecMulConstantMemory&lt;&lt;&lt;grids, blocks&gt;&gt;&gt;(d_At, d_B, d_C, nRow, nCol);
		else if (i == 3)
			MatVecMulSharedMemory&lt;&lt;&lt;grids, blocks, sizeof(lf) * blocks&gt;&gt;&gt;(d_At, d_B, d_C, nRow, nCol);
		cudaDeviceSynchronize();
		cudaEventRecord(end, 0);
		cudaEventSynchronize(beg);
		cudaEventSynchronize(end);
		lf elapsed_time;
		cudaEventElapsedTime(&amp;elapsed_time, beg, end);
		printf("%fms\n", elapsed_time);
	}

	cudaFree(d_A);
	cudaFree(d_At);
	cudaFree(d_B);
	cudaFree(d_C);
	free(h_A);
	free(h_At);
	free(h_B);
	free(h_C);
}
</code></pre>
:ET